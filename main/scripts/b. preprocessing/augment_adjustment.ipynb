{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bugi\\miniconda3\\envs\\env_skripsi\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bugi\\miniconda3\\envs\\env_skripsi\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(os.environ.get('PATH_CUSTOM_MODULES'))\n",
    "\n",
    "import augment_image\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare all basic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = os.environ.get('PATH_DATASET_DESTINATION')\n",
    "scenario_names = ['scenario_2', 'scenario_3'] # scenario 1 is the original dataset\n",
    "dataset_names = ['rimone', 'g1020', 'refuge', 'papila']\n",
    "fold_names = ['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']\n",
    "labels_name = ['normal', 'glaukoma']\n",
    "image_size = {'rimone': (300,300),\n",
    "            'g1020': (240,300),\n",
    "            'refuge': (300,300),\n",
    "            'papila': (200,300)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the path source and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge path source and path destination\n",
    "# for each dataset, scenario, and label\n",
    "path_dataset_src = {}\n",
    "path_dataset_aug = {}\n",
    "path_dataset_merge = {}\n",
    "## create the source path for training data\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "                path_dataset_src[scenario + '_'\n",
    "                                + dataset + '_'\n",
    "                                + fold] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train')\n",
    "del scenario, dataset, fold\n",
    "## create the destination path a.k.a. augmented path for training data\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            for label in labels_name:\n",
    "                path_dataset_aug[scenario + '_'\n",
    "                                + dataset + '_'\n",
    "                                + fold + '_'\n",
    "                                + label] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train_augmented',\n",
    "                                                        label)\n",
    "del scenario, dataset, fold, label\n",
    "## create the merge path for training data\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            path_dataset_merge[scenario + '_'\n",
    "                                + dataset + '_'\n",
    "                                + fold] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train_merged')\n",
    "del scenario, dataset, fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator for scenario 3(only clahe)\n",
    "datagenerator_s3 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=augment_image.clahe_augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the merged directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: scenario_2_rimone_fold_1\n",
      "Directory already exists: scenario_2_rimone_fold_2\n",
      "Directory already exists: scenario_2_rimone_fold_3\n",
      "Directory already exists: scenario_2_rimone_fold_4\n",
      "Directory already exists: scenario_2_rimone_fold_5\n",
      "Directory already exists: scenario_2_g1020_fold_1\n",
      "Directory already exists: scenario_2_g1020_fold_2\n",
      "Directory already exists: scenario_2_g1020_fold_3\n",
      "Directory already exists: scenario_2_g1020_fold_4\n",
      "Directory already exists: scenario_2_g1020_fold_5\n",
      "Directory already exists: scenario_2_refuge_fold_1\n",
      "Directory already exists: scenario_2_refuge_fold_2\n",
      "Directory already exists: scenario_2_refuge_fold_3\n",
      "Directory already exists: scenario_2_refuge_fold_4\n",
      "Directory already exists: scenario_2_refuge_fold_5\n",
      "Directory already exists: scenario_2_papila_fold_1\n",
      "Directory already exists: scenario_2_papila_fold_2\n",
      "Directory already exists: scenario_2_papila_fold_3\n",
      "Directory already exists: scenario_2_papila_fold_4\n",
      "Directory already exists: scenario_2_papila_fold_5\n",
      "Directory already exists: scenario_3_rimone_fold_1\n",
      "Directory already exists: scenario_3_rimone_fold_2\n",
      "Directory already exists: scenario_3_rimone_fold_3\n",
      "Directory already exists: scenario_3_rimone_fold_4\n",
      "Directory already exists: scenario_3_rimone_fold_5\n",
      "Directory already exists: scenario_3_g1020_fold_1\n",
      "Directory already exists: scenario_3_g1020_fold_2\n",
      "Directory already exists: scenario_3_g1020_fold_3\n",
      "Directory already exists: scenario_3_g1020_fold_4\n",
      "Directory already exists: scenario_3_g1020_fold_5\n",
      "Directory already exists: scenario_3_refuge_fold_1\n",
      "Directory already exists: scenario_3_refuge_fold_2\n",
      "Directory already exists: scenario_3_refuge_fold_3\n",
      "Directory already exists: scenario_3_refuge_fold_4\n",
      "Directory already exists: scenario_3_refuge_fold_5\n",
      "Directory already exists: scenario_3_papila_fold_1\n",
      "Directory already exists: scenario_3_papila_fold_2\n",
      "Directory already exists: scenario_3_papila_fold_3\n",
      "Directory already exists: scenario_3_papila_fold_4\n",
      "Directory already exists: scenario_3_papila_fold_5\n"
     ]
    }
   ],
   "source": [
    "# create the directory for the augmented dataset\n",
    "directory_result = augment_image.create_directory(path_dict=path_dataset_merge)\n",
    "## print the result\n",
    "for key, values in directory_result.items():\n",
    "    if key == 'Already Exists' and values != []:\n",
    "        for value in values:\n",
    "            print('Directory already exists:', value)\n",
    "del key, values, value, directory_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the original and augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
