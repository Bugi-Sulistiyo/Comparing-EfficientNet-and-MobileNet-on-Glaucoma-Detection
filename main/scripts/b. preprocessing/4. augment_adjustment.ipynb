{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(os.environ.get('PATH_CUSTOM_MODULES'))\n",
    "\n",
    "import augment_image\n",
    "import data_prep\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare all basic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = os.environ.get('PATH_DATASET_DESTINATION')\n",
    "scenario_names = ['scenario_2', 'scenario_3'] # scenario 1 is the original dataset\n",
    "dataset_names = ['rimone', 'g1020', 'refuge', 'papila']\n",
    "fold_names = ['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']\n",
    "labels_name = ['normal', 'glaukoma']\n",
    "image_size = {'rimone': (300,300),\n",
    "            'g1020': (240,300),\n",
    "            'refuge': (300,300),\n",
    "            'papila': (200,300)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the path source and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge path source and path destination\n",
    "# for each dataset, scenario, and label\n",
    "path_dataset_src = {}\n",
    "path_dataset_aug = {}\n",
    "path_dataset_merge = {}\n",
    "\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            for label in labels_name:\n",
    "                ## create the source path for training data\n",
    "                path_dataset_src[f'{scenario}_'\n",
    "                                + f'{dataset}_'\n",
    "                                + f'{fold}_'\n",
    "                                + label] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train',\n",
    "                                                        label)\n",
    "                ## create the destination path a.k.a. augmented path for training data\n",
    "                path_dataset_aug[scenario + '_'\n",
    "                                + dataset + '_'\n",
    "                                + fold + '_'\n",
    "                                + label] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train_augmented',\n",
    "                                                        label)\n",
    "                ## create the merge path for training data\n",
    "                path_dataset_merge[f'{scenario}_'\n",
    "                                    + f'{dataset}_'\n",
    "                                    + f'{fold}_'\n",
    "                                    + label] = os.path.join(path_source,\n",
    "                                                            scenario,\n",
    "                                                            dataset,\n",
    "                                                            fold,\n",
    "                                                            'train_merged',\n",
    "                                                            label)\n",
    "del scenario, dataset, fold, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the merged directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory for the augmented dataset\n",
    "directory_result = augment_image.create_directory(path_dict=path_dataset_merge)\n",
    "\n",
    "## print the result\n",
    "for key, values in directory_result.items():\n",
    "    if key == 'Already Exists' and values != []:\n",
    "        for value in values:\n",
    "            print('Directory already exists:', value)\n",
    "        del value\n",
    "del key, values, directory_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the variables to store the file names\n",
    "original_files = {}\n",
    "augmented_files = {}\n",
    "# get file for the original image\n",
    "for key, value in path_dataset_src.items():\n",
    "    original_files[key] = data_prep.get_file_names(path=value)\n",
    "del key, value\n",
    "# get file for the augmented image\n",
    "for key, value in path_dataset_aug.items():\n",
    "        augmented_files[key] = data_prep.get_file_names(path=value)\n",
    "del key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the categories\n",
    "len(original_files) == len(augmented_files) == len(path_dataset_src) == len(path_dataset_aug) == len(path_dataset_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the images\n",
    "for key in original_files.keys():\n",
    "    if len(original_files[key]) != len(augmented_files[key]):\n",
    "        print(f'{key} are not equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the original and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image type</th>\n",
       "      <th>id</th>\n",
       "      <th>Already Exists</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [image type, id, Already Exists, Success]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataframe to store the result\n",
    "copy_result = {\n",
    "    'image type': [],\n",
    "    'id': [],\n",
    "    'Already Exists': [],\n",
    "    'Success': []\n",
    "}\n",
    "\n",
    "df_result = pd.DataFrame(copy_result)\n",
    "del copy_result\n",
    "\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the original images\n",
    "for key in path_dataset_src.keys():\n",
    "    result = data_prep.copy_files(source_path=path_dataset_src[key],\n",
    "                                destination_path=path_dataset_merge[key],\n",
    "                                file_names=original_files[key])\n",
    "    df_result.loc[len(df_result)] = ['original',\n",
    "                                    key,\n",
    "                                    len(result['Already Exists']),\n",
    "                                    len(result['Success'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the augmented images\n",
    "for key in path_dataset_aug.keys():\n",
    "    result = data_prep.copy_files(source_path=path_dataset_aug[key],\n",
    "                                destination_path=path_dataset_merge[key],\n",
    "                                file_names=augmented_files[key])\n",
    "    df_result.loc[len(df_result)] = ['augmented',\n",
    "                                    key,\n",
    "                                    len(result['Already Exists']),\n",
    "                                    len(result['Success'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image type</th>\n",
       "      <th>id</th>\n",
       "      <th>Already Exists</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [image type, id, Already Exists, Success]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the result\n",
    "df_result.loc[df_result['Already Exists'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the path with file name\n",
    "## define the variable to store the files path\n",
    "rm_files = {}\n",
    "## merge the original image path\n",
    "for key, value in path_dataset_src.items():\n",
    "    rm_files[f'ori_{key}'] = [os.path.join(value,\n",
    "                                        file) for file in original_files[key]]\n",
    "del key, value\n",
    "# merge the augmented image path\n",
    "for key, value in path_dataset_aug.items():\n",
    "    rm_files[f'aug_{key}'] = [os.path.join(value,\n",
    "                                        file) for file in augmented_files[key]]\n",
    "del key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the length of the files\n",
    "len(rm_files) == len(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove all the files:\n",
      "Files removed: 0\n",
      "Files not removed: 109\n"
     ]
    }
   ],
   "source": [
    "for files in rm_files.values():\n",
    "    result_status = augment_image.remove_file(files)\n",
    "\n",
    "print('Remove all the files:',\n",
    "        f'Files removed: {len(result_status[\"Success\"])}',\n",
    "        f'Files not removed: {len(result_status[\"Not Found\"])}',\n",
    "        sep='\\n')\n",
    "\n",
    "del files, result_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the source directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the variable\n",
    "rm_dir = []\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            for data_type in ['train', 'train_augmented']:\n",
    "                rm_dir.append(os.path.join(path_source,\n",
    "                                        scenario,\n",
    "                                        dataset,\n",
    "                                        fold,\n",
    "                                        data_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove all the directory:\n",
      "Directory removed: 80\n",
      "Directory not removed: 0\n"
     ]
    }
   ],
   "source": [
    "# remove the directory\n",
    "result_status = augment_image.remove_dir(dir_path=rm_dir)\n",
    "\n",
    "print('Remove all the directory:',\n",
    "        f'Directory removed: {len(result_status[\"Success\"])}',\n",
    "        f'Directory not removed: {len(result_status[\"Not Found\"])}',\n",
    "        sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
