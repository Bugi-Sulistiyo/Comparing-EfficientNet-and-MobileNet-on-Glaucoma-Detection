{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bugi\\miniconda3\\envs\\env_skripsi\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bugi\\miniconda3\\envs\\env_skripsi\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(os.environ.get('PATH_CUSTOM_MODULES'))\n",
    "\n",
    "import augment_image\n",
    "import data_prep\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare all basic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = os.environ.get('PATH_DATASET_DESTINATION')\n",
    "scenario_names = ['scenario_2', 'scenario_3'] # scenario 1 is the original dataset\n",
    "dataset_names = ['rimone', 'g1020', 'refuge', 'papila']\n",
    "fold_names = ['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']\n",
    "labels_name = ['normal', 'glaukoma']\n",
    "image_size = {'rimone': (300,300),\n",
    "            'g1020': (240,300),\n",
    "            'refuge': (300,300),\n",
    "            'papila': (200,300)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the path source and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge path source and path destination\n",
    "# for each dataset, scenario, and label\n",
    "path_dataset_src = {}\n",
    "path_dataset_aug = {}\n",
    "path_dataset_merge = {}\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            for label in labels_name:\n",
    "                ## create the source path for training data\n",
    "                path_dataset_src[f'{scenario}_'\n",
    "                                + f'{dataset}_'\n",
    "                                + f'{fold}_'\n",
    "                                + label] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train',\n",
    "                                                        label)\n",
    "                ## create the destination path a.k.a. augmented path for training data\n",
    "                path_dataset_aug[scenario + '_'\n",
    "                                + dataset + '_'\n",
    "                                + fold + '_'\n",
    "                                + label] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train_augmented',\n",
    "                                                        label)\n",
    "                ## create the merge path for training data\n",
    "                path_dataset_merge[f'{scenario}_'\n",
    "                                    + f'{dataset}_'\n",
    "                                    + f'{fold}_'\n",
    "                                    + label] = os.path.join(path_source,\n",
    "                                                            scenario,\n",
    "                                                            dataset,\n",
    "                                                            fold,\n",
    "                                                            'train_merged',\n",
    "                                                            label)\n",
    "del scenario, dataset, fold, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator for scenario 3(only clahe)\n",
    "datagenerator_s3 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=augment_image.clahe_augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the merged directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: scenario_2_rimone_fold_1_normal\n",
      "Directory already exists: scenario_2_rimone_fold_1_glaukoma\n",
      "Directory already exists: scenario_2_rimone_fold_2_normal\n",
      "Directory already exists: scenario_2_rimone_fold_2_glaukoma\n",
      "Directory already exists: scenario_2_rimone_fold_3_normal\n",
      "Directory already exists: scenario_2_rimone_fold_3_glaukoma\n",
      "Directory already exists: scenario_2_rimone_fold_4_normal\n",
      "Directory already exists: scenario_2_rimone_fold_4_glaukoma\n",
      "Directory already exists: scenario_2_rimone_fold_5_normal\n",
      "Directory already exists: scenario_2_rimone_fold_5_glaukoma\n",
      "Directory already exists: scenario_2_g1020_fold_1_normal\n",
      "Directory already exists: scenario_2_g1020_fold_1_glaukoma\n",
      "Directory already exists: scenario_2_g1020_fold_2_normal\n",
      "Directory already exists: scenario_2_g1020_fold_2_glaukoma\n",
      "Directory already exists: scenario_2_g1020_fold_3_normal\n",
      "Directory already exists: scenario_2_g1020_fold_3_glaukoma\n",
      "Directory already exists: scenario_2_g1020_fold_4_normal\n",
      "Directory already exists: scenario_2_g1020_fold_4_glaukoma\n",
      "Directory already exists: scenario_2_g1020_fold_5_normal\n",
      "Directory already exists: scenario_2_g1020_fold_5_glaukoma\n",
      "Directory already exists: scenario_2_refuge_fold_1_normal\n",
      "Directory already exists: scenario_2_refuge_fold_1_glaukoma\n",
      "Directory already exists: scenario_2_refuge_fold_2_normal\n",
      "Directory already exists: scenario_2_refuge_fold_2_glaukoma\n",
      "Directory already exists: scenario_2_refuge_fold_3_normal\n",
      "Directory already exists: scenario_2_refuge_fold_3_glaukoma\n",
      "Directory already exists: scenario_2_refuge_fold_4_normal\n",
      "Directory already exists: scenario_2_refuge_fold_4_glaukoma\n",
      "Directory already exists: scenario_2_refuge_fold_5_normal\n",
      "Directory already exists: scenario_2_refuge_fold_5_glaukoma\n",
      "Directory already exists: scenario_2_papila_fold_1_normal\n",
      "Directory already exists: scenario_2_papila_fold_1_glaukoma\n",
      "Directory already exists: scenario_2_papila_fold_2_normal\n",
      "Directory already exists: scenario_2_papila_fold_2_glaukoma\n",
      "Directory already exists: scenario_2_papila_fold_3_normal\n",
      "Directory already exists: scenario_2_papila_fold_3_glaukoma\n",
      "Directory already exists: scenario_2_papila_fold_4_normal\n",
      "Directory already exists: scenario_2_papila_fold_4_glaukoma\n",
      "Directory already exists: scenario_2_papila_fold_5_normal\n",
      "Directory already exists: scenario_2_papila_fold_5_glaukoma\n",
      "Directory already exists: scenario_3_rimone_fold_1_normal\n",
      "Directory already exists: scenario_3_rimone_fold_1_glaukoma\n",
      "Directory already exists: scenario_3_rimone_fold_2_normal\n",
      "Directory already exists: scenario_3_rimone_fold_2_glaukoma\n",
      "Directory already exists: scenario_3_rimone_fold_3_normal\n",
      "Directory already exists: scenario_3_rimone_fold_3_glaukoma\n",
      "Directory already exists: scenario_3_rimone_fold_4_normal\n",
      "Directory already exists: scenario_3_rimone_fold_4_glaukoma\n",
      "Directory already exists: scenario_3_rimone_fold_5_normal\n",
      "Directory already exists: scenario_3_rimone_fold_5_glaukoma\n",
      "Directory already exists: scenario_3_g1020_fold_1_normal\n",
      "Directory already exists: scenario_3_g1020_fold_1_glaukoma\n",
      "Directory already exists: scenario_3_g1020_fold_2_normal\n",
      "Directory already exists: scenario_3_g1020_fold_2_glaukoma\n",
      "Directory already exists: scenario_3_g1020_fold_3_normal\n",
      "Directory already exists: scenario_3_g1020_fold_3_glaukoma\n",
      "Directory already exists: scenario_3_g1020_fold_4_normal\n",
      "Directory already exists: scenario_3_g1020_fold_4_glaukoma\n",
      "Directory already exists: scenario_3_g1020_fold_5_normal\n",
      "Directory already exists: scenario_3_g1020_fold_5_glaukoma\n",
      "Directory already exists: scenario_3_refuge_fold_1_normal\n",
      "Directory already exists: scenario_3_refuge_fold_1_glaukoma\n",
      "Directory already exists: scenario_3_refuge_fold_2_normal\n",
      "Directory already exists: scenario_3_refuge_fold_2_glaukoma\n",
      "Directory already exists: scenario_3_refuge_fold_3_normal\n",
      "Directory already exists: scenario_3_refuge_fold_3_glaukoma\n",
      "Directory already exists: scenario_3_refuge_fold_4_normal\n",
      "Directory already exists: scenario_3_refuge_fold_4_glaukoma\n",
      "Directory already exists: scenario_3_refuge_fold_5_normal\n",
      "Directory already exists: scenario_3_refuge_fold_5_glaukoma\n",
      "Directory already exists: scenario_3_papila_fold_1_normal\n",
      "Directory already exists: scenario_3_papila_fold_1_glaukoma\n",
      "Directory already exists: scenario_3_papila_fold_2_normal\n",
      "Directory already exists: scenario_3_papila_fold_2_glaukoma\n",
      "Directory already exists: scenario_3_papila_fold_3_normal\n",
      "Directory already exists: scenario_3_papila_fold_3_glaukoma\n",
      "Directory already exists: scenario_3_papila_fold_4_normal\n",
      "Directory already exists: scenario_3_papila_fold_4_glaukoma\n",
      "Directory already exists: scenario_3_papila_fold_5_normal\n",
      "Directory already exists: scenario_3_papila_fold_5_glaukoma\n"
     ]
    }
   ],
   "source": [
    "# create the directory for the augmented dataset\n",
    "directory_result = augment_image.create_directory(path_dict=path_dataset_merge)\n",
    "\n",
    "## print the result\n",
    "for key, values in directory_result.items():\n",
    "    if key == 'Already Exists' and values != []:\n",
    "        for value in values:\n",
    "            print('Directory already exists:', value)\n",
    "        value = ''\n",
    "del key, values, value, directory_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of images in the source directory\n",
    "original_files = {}\n",
    "augmented_files = {}\n",
    "## for the original image\n",
    "for key, value in path_dataset_src.items():\n",
    "    original_files[key] = data_prep.get_file_names(path=value)\n",
    "del key, value\n",
    "## for the augmented image\n",
    "for key, value in path_dataset_aug.items():\n",
    "        augmented_files[key] = data_prep.get_file_names(path=value)\n",
    "del key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data for each scenario\n",
    "s2_ori_files = {}\n",
    "s2_aug_files = {}\n",
    "s3_ori_files = {}\n",
    "s3_aug_files = {}\n",
    "\n",
    "## splitting the original files\n",
    "for key, value in original_files.items():\n",
    "    if 'scenario_2' in key:\n",
    "        s2_ori_files[key] = value\n",
    "    elif 'scenario_3' in key:\n",
    "        s3_ori_files[key] = value\n",
    "    else:\n",
    "        print('Error:', key)\n",
    "del key, value\n",
    "## splitting the augmented files\n",
    "for key, value in augmented_files.items():\n",
    "    if 'scenario_2' in key:\n",
    "        s2_aug_files[key] = value\n",
    "    elif 'scenario_3' in key:\n",
    "        s3_aug_files[key] = value\n",
    "    else:\n",
    "        print('Error:', key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the original and augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m## for the augmented image\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m path_dataset_aug\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m### copying the augmented file into the merged directory\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mdata_prep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_dataset_merge\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfile_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmented_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m### removing the previous augmented file\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     augment_image\u001b[38;5;241m.\u001b[39mremove_file(files_path\u001b[38;5;241m=\u001b[39m[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_dataset_merge[key],\n\u001b[0;32m     33\u001b[0m                                                         file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m augmented_files[key]])\n",
      "File \u001b[1;32md:\\\\Programming\\\\Python\\\\Skripsi\\\\main\\\\scripts\\\\action\\data_prep.py:89\u001b[0m, in \u001b[0;36mcopy_files\u001b[1;34m(source_path, destination_path, file_names)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m file_names:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m         result_status[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(destination_path, file_name))\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bugi\\miniconda3\\envs\\env_skripsi\\lib\\shutil.py:258\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m                 _fastcopy_fcopyfile(fsrc, fdst, posix\u001b[38;5;241m.\u001b[39m_COPYFILE_DATA)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# copy the image from the source to the destination\n",
    "copy_result = {\n",
    "    'image type': [],\n",
    "    'id': [],\n",
    "    'Already Exists': [],\n",
    "    'Success': []\n",
    "}\n",
    "\n",
    "## for the original image\n",
    "for key, value in path_dataset_src.items():\n",
    "    ### copying the original file into the merged directory\n",
    "    temp = data_prep.copy_files(source_path=value,\n",
    "                                destination_path=path_dataset_merge[key],\n",
    "                                file_names=original_files[key])\n",
    "    ### removing the previous original file\n",
    "    augment_image.remove_file(files_path=[os.path.join(path_dataset_merge[key],\n",
    "                                                        file) for file in original_files[key]])\n",
    "    \n",
    "    copy_result['image type'].append('original')\n",
    "    copy_result['id'].append(key)\n",
    "    copy_result['Already Exists'].append(len(temp['Already Exists']))\n",
    "    copy_result['Success'].append(len(temp['Success']))\n",
    "del key, value, temp\n",
    "\n",
    "## for the augmented image\n",
    "for key, value in path_dataset_aug.items():\n",
    "    ### copying the augmented file into the merged directory\n",
    "    temp = data_prep.copy_files(source_path=value,\n",
    "                                destination_path=path_dataset_merge[key],\n",
    "                                file_names=augmented_files[key])\n",
    "    ### removing the previous augmented file\n",
    "    augment_image.remove_file(files_path=[os.path.join(path_dataset_merge[key],\n",
    "                                                        file) for file in augmented_files[key]])\n",
    "    \n",
    "    copy_result['image type'].append('augmented')\n",
    "    copy_result['id'].append(key)\n",
    "    copy_result['Already Exists'].append(len(temp['Already Exists']))\n",
    "    copy_result['Success'].append(len(temp['Success']))\n",
    "del key, value, temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
