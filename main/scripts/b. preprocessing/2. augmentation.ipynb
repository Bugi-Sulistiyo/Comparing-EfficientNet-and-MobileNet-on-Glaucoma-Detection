{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(os.environ.get('PATH_CUSTOM_MODULES'))\n",
    "\n",
    "import augment_image\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare all basic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = os.environ.get('PATH_DATASET_DESTINATION')\n",
    "scenario_names = ['scenario_2', 'scenario_3']\n",
    "dataset_names = ['rimone', 'g1020', 'refuge', 'papila']\n",
    "fold_names = ['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']\n",
    "labels_name = ['normal', 'glaukoma']\n",
    "image_size = {'rimone': (300,300),\n",
    "            'g1020': (240,300),\n",
    "            'refuge': (300,300),\n",
    "            'papila': (200,300)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the path source and detination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge path source and path destination for each dataset, scenario, and label\n",
    "path_dataset_src = {}\n",
    "path_dataset_val_test_src = {}\n",
    "path_dataset_aug = {}\n",
    "path_dataset_clahe_dst = {}\n",
    "path_dataset_val_test_dest = {}\n",
    "\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            ## create the source path for training data\n",
    "            path_dataset_src[f'{scenario}_'\n",
    "                            + f'{dataset}_'\n",
    "                            + fold] = os.path.join(path_source,\n",
    "                                                    scenario,\n",
    "                                                    dataset,\n",
    "                                                    fold,\n",
    "                                                    'train')\n",
    "            \n",
    "            for data_type in ['val', 'test']:\n",
    "                for label in labels_name:\n",
    "                    ## create the destination path a.k.a. augmented path for validation and testing data\n",
    "                    path_dataset_val_test_dest[f'{scenario}_'\n",
    "                                                + f'{dataset}_'\n",
    "                                                + f'{fold}_'\n",
    "                                                + data_type + '_'\n",
    "                                                + label] = os.path.join(path_source,\n",
    "                                                                        scenario,\n",
    "                                                                        dataset,\n",
    "                                                                        fold,\n",
    "                                                                        data_type,\n",
    "                                                                        label)\n",
    "                    \n",
    "                ## create the source path for validation and testing data\n",
    "                path_dataset_val_test_src[f'{scenario}_'\n",
    "                                        + f'{dataset}_'\n",
    "                                        + f'{fold}_'\n",
    "                                        + data_type] = os.path.join(path_source,\n",
    "                                                                    scenario,\n",
    "                                                                    dataset,\n",
    "                                                                    fold,\n",
    "                                                                    data_type)\n",
    "del scenario, dataset, fold, data_type, label\n",
    "\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            for label in labels_name:\n",
    "                ## create the source path for training data with only clahe augmentation\n",
    "                path_dataset_clahe_dst[f'{scenario}_'\n",
    "                                    + f'{dataset}_'\n",
    "                                    + f'{fold}_'\n",
    "                                    + label] = os.path.join(path_source,\n",
    "                                                            scenario,\n",
    "                                                            dataset,\n",
    "                                                            fold,\n",
    "                                                            'train',\n",
    "                                                            label)\n",
    "                \n",
    "                ## create the destination path a.k.a. augmented path for training data\n",
    "                path_dataset_aug[f'{scenario}_'\n",
    "                                + f'{dataset}_'\n",
    "                                + f'{fold}_'\n",
    "                                + label] = os.path.join(path_source,\n",
    "                                                        scenario,\n",
    "                                                        dataset,\n",
    "                                                        fold,\n",
    "                                                        'train_augmented',\n",
    "                                                        label)\n",
    "del scenario, dataset, fold, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the image data generator for each scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the image data generator\n",
    "## data generator for scenario 2 (with augmentation)\n",
    "datagenerator_s2 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[1, 1.5]\n",
    ")\n",
    "## data generator for scenario 3 (with augmentation and clahe)\n",
    "datagenerator_s3 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[1, 1.5],\n",
    "    preprocessing_function=augment_image.clahe_augmentation\n",
    ")\n",
    "## data generator for scenario 3(only clahe)\n",
    "datagenerator_s3_clahe = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=augment_image.clahe_augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Augment Directory\n",
    "only run this code once to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory for the augmented dataset\n",
    "directory_result = augment_image.create_directory(path_dict=path_dataset_aug)\n",
    "## print the result\n",
    "for key, values in directory_result.items():\n",
    "    if key == 'Already Exists' and values != []:\n",
    "        for value in values:\n",
    "            print('Directory already exists:', value)\n",
    "        del value\n",
    "del key, values, directory_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "**Condition**:\n",
    "- basic augmentation, \n",
    "- rgb color\n",
    "- no clahe\n",
    "#### Import the image into data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the augmentation mode for scenario 2\n",
    "s2_src = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rimone fold_1 normal\n",
      "Found 218 images belonging to 1 classes.\n",
      "rimone fold_1 glaukoma\n",
      "Found 121 images belonging to 1 classes.\n",
      "rimone fold_2 normal\n",
      "Found 218 images belonging to 1 classes.\n",
      "rimone fold_2 glaukoma\n",
      "Found 121 images belonging to 1 classes.\n",
      "rimone fold_3 normal\n",
      "Found 218 images belonging to 1 classes.\n",
      "rimone fold_3 glaukoma\n",
      "Found 121 images belonging to 1 classes.\n",
      "rimone fold_4 normal\n",
      "Found 219 images belonging to 1 classes.\n",
      "rimone fold_4 glaukoma\n",
      "Found 120 images belonging to 1 classes.\n",
      "rimone fold_5 normal\n",
      "Found 219 images belonging to 1 classes.\n",
      "rimone fold_5 glaukoma\n",
      "Found 120 images belonging to 1 classes.\n",
      "g1020 fold_1 normal\n",
      "Found 506 images belonging to 1 classes.\n",
      "g1020 fold_1 glaukoma\n",
      "Found 208 images belonging to 1 classes.\n",
      "g1020 fold_2 normal\n",
      "Found 506 images belonging to 1 classes.\n",
      "g1020 fold_2 glaukoma\n",
      "Found 208 images belonging to 1 classes.\n",
      "g1020 fold_3 normal\n",
      "Found 506 images belonging to 1 classes.\n",
      "g1020 fold_3 glaukoma\n",
      "Found 208 images belonging to 1 classes.\n",
      "g1020 fold_4 normal\n",
      "Found 506 images belonging to 1 classes.\n",
      "g1020 fold_4 glaukoma\n",
      "Found 208 images belonging to 1 classes.\n",
      "g1020 fold_5 normal\n",
      "Found 507 images belonging to 1 classes.\n",
      "g1020 fold_5 glaukoma\n",
      "Found 207 images belonging to 1 classes.\n",
      "refuge fold_1 normal\n",
      "Found 756 images belonging to 1 classes.\n",
      "refuge fold_1 glaukoma\n",
      "Found 84 images belonging to 1 classes.\n",
      "refuge fold_2 normal\n",
      "Found 756 images belonging to 1 classes.\n",
      "refuge fold_2 glaukoma\n",
      "Found 84 images belonging to 1 classes.\n",
      "refuge fold_3 normal\n",
      "Found 756 images belonging to 1 classes.\n",
      "refuge fold_3 glaukoma\n",
      "Found 84 images belonging to 1 classes.\n",
      "refuge fold_4 normal\n",
      "Found 756 images belonging to 1 classes.\n",
      "refuge fold_4 glaukoma\n",
      "Found 84 images belonging to 1 classes.\n",
      "refuge fold_5 normal\n",
      "Found 756 images belonging to 1 classes.\n",
      "refuge fold_5 glaukoma\n",
      "Found 84 images belonging to 1 classes.\n",
      "papila fold_1 normal\n",
      "Found 232 images belonging to 1 classes.\n",
      "papila fold_1 glaukoma\n",
      "Found 109 images belonging to 1 classes.\n",
      "papila fold_2 normal\n",
      "Found 232 images belonging to 1 classes.\n",
      "papila fold_2 glaukoma\n",
      "Found 109 images belonging to 1 classes.\n",
      "papila fold_3 normal\n",
      "Found 232 images belonging to 1 classes.\n",
      "papila fold_3 glaukoma\n",
      "Found 109 images belonging to 1 classes.\n",
      "papila fold_4 normal\n",
      "Found 233 images belonging to 1 classes.\n",
      "papila fold_4 glaukoma\n",
      "Found 109 images belonging to 1 classes.\n",
      "papila fold_5 normal\n",
      "Found 233 images belonging to 1 classes.\n",
      "papila fold_5 glaukoma\n",
      "Found 109 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# get the image using image data generator\n",
    "## load image using image data generator\n",
    "for dataset in dataset_names:\n",
    "    for fold in fold_names:\n",
    "        for label in labels_name:\n",
    "            print(f'{dataset} {fold} {label}')\n",
    "            s2_src[dataset + '_'\n",
    "                    + fold + '_'\n",
    "                    + label] = (datagenerator_s2.flow_from_directory(\n",
    "                                path_dataset_src[scenario_names[0] + '_'\n",
    "                                                + dataset + '_'\n",
    "                                                + fold],\n",
    "                                target_size=image_size[dataset],\n",
    "                                class_mode='binary',\n",
    "                                classes=[label],\n",
    "                                shuffle=True,\n",
    "                                seed=1915026018,\n",
    "                                save_to_dir=path_dataset_aug[scenario_names[0] + '_'\n",
    "                                                            + dataset + '_'\n",
    "                                                            + fold + '_'\n",
    "                                                            + label],\n",
    "                                save_prefix=f's2_{dataset}_f{fold.split(\"_\")[-1]}_{label}',\n",
    "                                save_format='jpg'))\n",
    "del dataset, fold, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the augmented image & saved it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented image for rimone f1 normal...\n",
      "Elapsed time: 5.95 seconds\n",
      "Generating augmented image for rimone f1 glaukoma...\n",
      "Elapsed time: 3.17 seconds\n",
      "Generating augmented image for rimone f2 normal...\n",
      "Elapsed time: 6.89 seconds\n",
      "Generating augmented image for rimone f2 glaukoma...\n",
      "Elapsed time: 4.31 seconds\n",
      "Generating augmented image for rimone f3 normal...\n",
      "Elapsed time: 7.91 seconds\n",
      "Generating augmented image for rimone f3 glaukoma...\n",
      "Elapsed time: 9.65 seconds\n",
      "Generating augmented image for rimone f4 normal...\n",
      "Elapsed time: 12.46 seconds\n",
      "Generating augmented image for rimone f4 glaukoma...\n",
      "Elapsed time: 6.17 seconds\n",
      "Generating augmented image for rimone f5 normal...\n",
      "Elapsed time: 9.67 seconds\n",
      "Generating augmented image for rimone f5 glaukoma...\n",
      "Elapsed time: 5.32 seconds\n",
      "Generating augmented image for g1020 f1 normal...\n",
      "Elapsed time: 89.51 seconds\n",
      "Generating augmented image for g1020 f1 glaukoma...\n",
      "Elapsed time: 25.73 seconds\n",
      "Generating augmented image for g1020 f2 normal...\n",
      "Elapsed time: 54.16 seconds\n",
      "Generating augmented image for g1020 f2 glaukoma...\n",
      "Elapsed time: 21.00 seconds\n",
      "Generating augmented image for g1020 f3 normal...\n",
      "Elapsed time: 53.15 seconds\n",
      "Generating augmented image for g1020 f3 glaukoma...\n",
      "Elapsed time: 28.80 seconds\n",
      "Generating augmented image for g1020 f4 normal...\n",
      "Elapsed time: 73.95 seconds\n",
      "Generating augmented image for g1020 f4 glaukoma...\n",
      "Elapsed time: 21.95 seconds\n",
      "Generating augmented image for g1020 f5 normal...\n",
      "Elapsed time: 52.63 seconds\n",
      "Generating augmented image for g1020 f5 glaukoma...\n",
      "Elapsed time: 25.28 seconds\n",
      "Generating augmented image for refuge f1 normal...\n",
      "Elapsed time: 71.04 seconds\n",
      "Generating augmented image for refuge f1 glaukoma...\n",
      "Elapsed time: 7.60 seconds\n",
      "Generating augmented image for refuge f2 normal...\n",
      "Elapsed time: 79.49 seconds\n",
      "Generating augmented image for refuge f2 glaukoma...\n",
      "Elapsed time: 9.63 seconds\n",
      "Generating augmented image for refuge f3 normal...\n",
      "Elapsed time: 83.34 seconds\n",
      "Generating augmented image for refuge f3 glaukoma...\n",
      "Elapsed time: 9.35 seconds\n",
      "Generating augmented image for refuge f4 normal...\n",
      "Elapsed time: 44.28 seconds\n",
      "Generating augmented image for refuge f4 glaukoma...\n",
      "Elapsed time: 5.63 seconds\n",
      "Generating augmented image for refuge f5 normal...\n",
      "Elapsed time: 35.27 seconds\n",
      "Generating augmented image for refuge f5 glaukoma...\n",
      "Elapsed time: 4.12 seconds\n",
      "Generating augmented image for papila f1 normal...\n",
      "Elapsed time: 11.91 seconds\n",
      "Generating augmented image for papila f1 glaukoma...\n",
      "Elapsed time: 4.86 seconds\n",
      "Generating augmented image for papila f2 normal...\n",
      "Elapsed time: 10.07 seconds\n",
      "Generating augmented image for papila f2 glaukoma...\n",
      "Elapsed time: 4.85 seconds\n",
      "Generating augmented image for papila f3 normal...\n",
      "Elapsed time: 9.78 seconds\n",
      "Generating augmented image for papila f3 glaukoma...\n",
      "Elapsed time: 4.96 seconds\n",
      "Generating augmented image for papila f4 normal...\n",
      "Elapsed time: 18.04 seconds\n",
      "Generating augmented image for papila f4 glaukoma...\n",
      "Elapsed time: 6.99 seconds\n",
      "Generating augmented image for papila f5 normal...\n",
      "Elapsed time: 10.60 seconds\n",
      "Generating augmented image for papila f5 glaukoma...\n",
      "Elapsed time: 4.91 seconds\n"
     ]
    }
   ],
   "source": [
    "augment_image.generate_aug_img(dataset_names=dataset_names,\n",
    "                                fold_names=fold_names,\n",
    "                                labels_names=labels_name,\n",
    "                                batch_datasets=s2_src,\n",
    "                                data_type='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a varible to store the file name\n",
    "s2_src_fname = {}\n",
    "s2_aug_fname = {}\n",
    "\n",
    "# collecting the source file name\n",
    "for key, value in path_dataset_src.items():\n",
    "    if key.split('_')[1] == '2':\n",
    "        for label in labels_name:\n",
    "            s2_src_fname[key + '_'\n",
    "                        + label] = [file for file in os.listdir(os.path.join(value,label))]\n",
    "del key, value, label\n",
    "\n",
    "# collecting the augmented file name\n",
    "for key, value in path_dataset_aug.items():\n",
    "    if key.split('_')[1] == '2':\n",
    "        s2_aug_fname[key] = [file for file in os.listdir(value)]\n",
    "del key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>file_count</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scenario_2_rimone_fold_1_normal</td>\n",
       "      <td>218</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scenario_2_rimone_fold_1_glaukoma</td>\n",
       "      <td>121</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            category  file_count    type\n",
       "0    scenario_2_rimone_fold_1_normal         218  source\n",
       "1  scenario_2_rimone_fold_1_glaukoma         121  source"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the two dictionary into dataframe\n",
    "s2_df_result = pd.concat([pd.DataFrame({\n",
    "                                'category': s2_src_fname.keys(),\n",
    "                                'file_count': [len(value) for value in s2_src_fname.values()],\n",
    "                                'type': 'source'\n",
    "                            }),\n",
    "                            pd.DataFrame({\n",
    "                                'category': s2_aug_fname.keys(),\n",
    "                                'file_count': [len(value) for value in s2_aug_fname.values()],\n",
    "                                'type': 'augmented'\n",
    "                            })])\n",
    "s2_df_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>augmented</th>\n",
       "      <th>source</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scenario_2_g1020_fold_1_glaukoma</th>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_2_g1020_fold_1_normal</th>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_2_g1020_fold_2_glaukoma</th>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_2_g1020_fold_2_normal</th>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_2_g1020_fold_3_glaukoma</th>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type                              augmented  source status\n",
       "category                                                  \n",
       "scenario_2_g1020_fold_1_glaukoma      208.0   208.0  valid\n",
       "scenario_2_g1020_fold_1_normal        506.0   506.0  valid\n",
       "scenario_2_g1020_fold_2_glaukoma      208.0   208.0  valid\n",
       "scenario_2_g1020_fold_2_normal        506.0   506.0  valid\n",
       "scenario_2_g1020_fold_3_glaukoma      208.0   208.0  valid"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the file count\n",
    "s2_df_validate = pd.DataFrame(s2_df_result.groupby(['category', 'type']).file_count.sum())\n",
    "\n",
    "s2_df_validate.sort_values(by='category', inplace=True)\n",
    "s2_df_validate = s2_df_validate.pivot_table(index='category',\n",
    "                                            columns='type',\n",
    "                                            values='file_count')\n",
    "\n",
    "s2_df_validate.loc[s2_df_validate.augmented == s2_df_validate.source,\n",
    "                    'status'] = 'valid'\n",
    "s2_df_validate.loc[s2_df_validate.augmented != s2_df_validate.source,\n",
    "                    'status'] = 'invalid'\n",
    "\n",
    "s2_df_validate.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 40\n",
      "\n",
      "Total valid categories: 40\n",
      "\n",
      "Total invalid categories: 0\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(f'Total categories: {len(s2_df_validate)}',\n",
    "    f'\\nTotal valid categories: {len(s2_df_validate[s2_df_validate.status == \"valid\"])}',\n",
    "    f'\\nTotal invalid categories: {len(s2_df_validate[s2_df_validate.status == \"invalid\"])}',\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "**Condition**:\n",
    "- basic augmentation, \n",
    "- rgb color, \n",
    "- clahe\n",
    "#### Import the image into data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the augmentation mode for scenario 3\n",
    "s3_src = {}\n",
    "s3_tclahe_src = {}\n",
    "s3_val_test_src = {}\n",
    "s3_cfile_val_test = []\n",
    "s3_cfile_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rimone f1 normal...\n",
      "Found 218 images belonging to 1 classes.\n",
      "Loading rimone f1 glaukoma...\n",
      "Found 121 images belonging to 1 classes.\n",
      "Loading rimone f2 normal...\n",
      "Found 218 images belonging to 1 classes.\n",
      "Loading rimone f2 glaukoma...\n",
      "Found 121 images belonging to 1 classes.\n",
      "Loading rimone f3 normal...\n",
      "Found 218 images belonging to 1 classes.\n",
      "Loading rimone f3 glaukoma...\n",
      "Found 121 images belonging to 1 classes.\n",
      "Loading rimone f4 normal...\n",
      "Found 219 images belonging to 1 classes.\n",
      "Loading rimone f4 glaukoma...\n",
      "Found 120 images belonging to 1 classes.\n",
      "Loading rimone f5 normal...\n",
      "Found 219 images belonging to 1 classes.\n",
      "Loading rimone f5 glaukoma...\n",
      "Found 120 images belonging to 1 classes.\n",
      "Loading g1020 f1 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f1 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f2 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f2 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f3 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f3 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f4 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f4 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f5 normal...\n",
      "Found 507 images belonging to 1 classes.\n",
      "Loading g1020 f5 glaukoma...\n",
      "Found 207 images belonging to 1 classes.\n",
      "Loading refuge f1 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f1 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f2 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f2 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f3 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f3 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f4 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f4 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f5 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f5 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading papila f1 normal...\n",
      "Found 232 images belonging to 1 classes.\n",
      "Loading papila f1 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f2 normal...\n",
      "Found 232 images belonging to 1 classes.\n",
      "Loading papila f2 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f3 normal...\n",
      "Found 232 images belonging to 1 classes.\n",
      "Loading papila f3 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f4 normal...\n",
      "Found 233 images belonging to 1 classes.\n",
      "Loading papila f4 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f5 normal...\n",
      "Found 233 images belonging to 1 classes.\n",
      "Loading papila f5 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# load image using image data generator for training data\n",
    "for dataset in dataset_names:\n",
    "    for fold in fold_names:\n",
    "        for label in labels_name:\n",
    "            print(f'Loading {dataset} f{fold.split(\"_\")[-1]} {label}...')\n",
    "            s3_src[dataset + '_'\n",
    "                    + fold + '_'\n",
    "                    + label] = (datagenerator_s3.flow_from_directory(\n",
    "                                path_dataset_src[scenario_names[1] + '_'\n",
    "                                                + dataset + '_'\n",
    "                                                + fold],\n",
    "                                target_size=image_size[dataset],\n",
    "                                class_mode='binary',\n",
    "                                classes=[label],\n",
    "                                shuffle=True,\n",
    "                                seed=1915026018,\n",
    "                                save_to_dir=path_dataset_aug[scenario_names[1] + '_'\n",
    "                                                            + dataset + '_'\n",
    "                                                            + fold + '_'\n",
    "                                                            + label],\n",
    "                                save_prefix=f's3_{dataset}_f{fold.split(\"_\")[-1]}_{label}',\n",
    "                                save_format='jpg'))\n",
    "del dataset, fold, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rimone f1 normal...\n",
      "Found 218 images belonging to 1 classes.\n",
      "Loading rimone f1 glaukoma...\n",
      "Found 121 images belonging to 1 classes.\n",
      "Loading rimone f2 normal...\n",
      "Found 218 images belonging to 1 classes.\n",
      "Loading rimone f2 glaukoma...\n",
      "Found 121 images belonging to 1 classes.\n",
      "Loading rimone f3 normal...\n",
      "Found 218 images belonging to 1 classes.\n",
      "Loading rimone f3 glaukoma...\n",
      "Found 121 images belonging to 1 classes.\n",
      "Loading rimone f4 normal...\n",
      "Found 219 images belonging to 1 classes.\n",
      "Loading rimone f4 glaukoma...\n",
      "Found 120 images belonging to 1 classes.\n",
      "Loading rimone f5 normal...\n",
      "Found 219 images belonging to 1 classes.\n",
      "Loading rimone f5 glaukoma...\n",
      "Found 120 images belonging to 1 classes.\n",
      "Loading g1020 f1 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f1 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f2 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f2 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f3 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f3 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f4 normal...\n",
      "Found 506 images belonging to 1 classes.\n",
      "Loading g1020 f4 glaukoma...\n",
      "Found 208 images belonging to 1 classes.\n",
      "Loading g1020 f5 normal...\n",
      "Found 507 images belonging to 1 classes.\n",
      "Loading g1020 f5 glaukoma...\n",
      "Found 207 images belonging to 1 classes.\n",
      "Loading refuge f1 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f1 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f2 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f2 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f3 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f3 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f4 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f4 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading refuge f5 normal...\n",
      "Found 756 images belonging to 1 classes.\n",
      "Loading refuge f5 glaukoma...\n",
      "Found 84 images belonging to 1 classes.\n",
      "Loading papila f1 normal...\n",
      "Found 232 images belonging to 1 classes.\n",
      "Loading papila f1 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f2 normal...\n",
      "Found 232 images belonging to 1 classes.\n",
      "Loading papila f2 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f3 normal...\n",
      "Found 232 images belonging to 1 classes.\n",
      "Loading papila f3 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f4 normal...\n",
      "Found 233 images belonging to 1 classes.\n",
      "Loading papila f4 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n",
      "Loading papila f5 normal...\n",
      "Found 233 images belonging to 1 classes.\n",
      "Loading papila f5 glaukoma...\n",
      "Found 109 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# load image using image data generator for training data with only clahe augmentation\n",
    "for dataset in dataset_names:\n",
    "    for fold in fold_names:\n",
    "        for label in labels_name:\n",
    "            print(f'Loading {dataset} f{fold.split(\"_\")[-1]} {label}...')\n",
    "            s3_tclahe_src[dataset + '_'\n",
    "                    + fold + '_'\n",
    "                    + label] = (datagenerator_s3_clahe.flow_from_directory(\n",
    "                                path_dataset_src[scenario_names[1] + '_'\n",
    "                                                + dataset + '_'\n",
    "                                                + fold],\n",
    "                                target_size=image_size[dataset],\n",
    "                                class_mode='binary',\n",
    "                                classes=[label],\n",
    "                                shuffle=True,\n",
    "                                seed=1915026018,\n",
    "                                save_to_dir=path_dataset_clahe_dst[scenario_names[1] + '_'\n",
    "                                                            + dataset + '_'\n",
    "                                                            + fold + '_'\n",
    "                                                            + label],\n",
    "                                save_prefix=f's3_{dataset}_f{fold.split(\"_\")[-1]}_clahe_{label}',\n",
    "                                save_format='jpg'))\n",
    "            s3_cfile_train.append(f's3_{dataset}_f{fold.split(\"_\")[-1]}_clahe_{label}')\n",
    "del dataset, fold, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rimone fold_1 val normal\n",
      "Found 32 images belonging to 1 classes.\n",
      "rimone fold_1 val glaukoma\n",
      "Found 17 images belonging to 1 classes.\n",
      "rimone fold_1 test normal\n",
      "Found 63 images belonging to 1 classes.\n",
      "rimone fold_1 test glaukoma\n",
      "Found 34 images belonging to 1 classes.\n",
      "rimone fold_2 val normal\n",
      "Found 32 images belonging to 1 classes.\n",
      "rimone fold_2 val glaukoma\n",
      "Found 17 images belonging to 1 classes.\n",
      "rimone fold_2 test normal\n",
      "Found 63 images belonging to 1 classes.\n",
      "rimone fold_2 test glaukoma\n",
      "Found 34 images belonging to 1 classes.\n",
      "rimone fold_3 val normal\n",
      "Found 32 images belonging to 1 classes.\n",
      "rimone fold_3 val glaukoma\n",
      "Found 17 images belonging to 1 classes.\n",
      "rimone fold_3 test normal\n",
      "Found 63 images belonging to 1 classes.\n",
      "rimone fold_3 test glaukoma\n",
      "Found 34 images belonging to 1 classes.\n",
      "rimone fold_4 val normal\n",
      "Found 32 images belonging to 1 classes.\n",
      "rimone fold_4 val glaukoma\n",
      "Found 17 images belonging to 1 classes.\n",
      "rimone fold_4 test normal\n",
      "Found 62 images belonging to 1 classes.\n",
      "rimone fold_4 test glaukoma\n",
      "Found 35 images belonging to 1 classes.\n",
      "rimone fold_5 val normal\n",
      "Found 32 images belonging to 1 classes.\n",
      "rimone fold_5 val glaukoma\n",
      "Found 17 images belonging to 1 classes.\n",
      "rimone fold_5 test normal\n",
      "Found 62 images belonging to 1 classes.\n",
      "rimone fold_5 test glaukoma\n",
      "Found 35 images belonging to 1 classes.\n",
      "g1020 fold_1 val normal\n",
      "Found 73 images belonging to 1 classes.\n",
      "g1020 fold_1 val glaukoma\n",
      "Found 29 images belonging to 1 classes.\n",
      "g1020 fold_1 test normal\n",
      "Found 145 images belonging to 1 classes.\n",
      "g1020 fold_1 test glaukoma\n",
      "Found 59 images belonging to 1 classes.\n",
      "g1020 fold_2 val normal\n",
      "Found 73 images belonging to 1 classes.\n",
      "g1020 fold_2 val glaukoma\n",
      "Found 29 images belonging to 1 classes.\n",
      "g1020 fold_2 test normal\n",
      "Found 145 images belonging to 1 classes.\n",
      "g1020 fold_2 test glaukoma\n",
      "Found 59 images belonging to 1 classes.\n",
      "g1020 fold_3 val normal\n",
      "Found 73 images belonging to 1 classes.\n",
      "g1020 fold_3 val glaukoma\n",
      "Found 29 images belonging to 1 classes.\n",
      "g1020 fold_3 test normal\n",
      "Found 145 images belonging to 1 classes.\n",
      "g1020 fold_3 test glaukoma\n",
      "Found 59 images belonging to 1 classes.\n",
      "g1020 fold_4 val normal\n",
      "Found 73 images belonging to 1 classes.\n",
      "g1020 fold_4 val glaukoma\n",
      "Found 29 images belonging to 1 classes.\n",
      "g1020 fold_4 test normal\n",
      "Found 145 images belonging to 1 classes.\n",
      "g1020 fold_4 test glaukoma\n",
      "Found 59 images belonging to 1 classes.\n",
      "g1020 fold_5 val normal\n",
      "Found 73 images belonging to 1 classes.\n",
      "g1020 fold_5 val glaukoma\n",
      "Found 29 images belonging to 1 classes.\n",
      "g1020 fold_5 test normal\n",
      "Found 144 images belonging to 1 classes.\n",
      "g1020 fold_5 test glaukoma\n",
      "Found 60 images belonging to 1 classes.\n",
      "refuge fold_1 val normal\n",
      "Found 108 images belonging to 1 classes.\n",
      "refuge fold_1 val glaukoma\n",
      "Found 12 images belonging to 1 classes.\n",
      "refuge fold_1 test normal\n",
      "Found 216 images belonging to 1 classes.\n",
      "refuge fold_1 test glaukoma\n",
      "Found 24 images belonging to 1 classes.\n",
      "refuge fold_2 val normal\n",
      "Found 108 images belonging to 1 classes.\n",
      "refuge fold_2 val glaukoma\n",
      "Found 12 images belonging to 1 classes.\n",
      "refuge fold_2 test normal\n",
      "Found 216 images belonging to 1 classes.\n",
      "refuge fold_2 test glaukoma\n",
      "Found 24 images belonging to 1 classes.\n",
      "refuge fold_3 val normal\n",
      "Found 108 images belonging to 1 classes.\n",
      "refuge fold_3 val glaukoma\n",
      "Found 12 images belonging to 1 classes.\n",
      "refuge fold_3 test normal\n",
      "Found 216 images belonging to 1 classes.\n",
      "refuge fold_3 test glaukoma\n",
      "Found 24 images belonging to 1 classes.\n",
      "refuge fold_4 val normal\n",
      "Found 108 images belonging to 1 classes.\n",
      "refuge fold_4 val glaukoma\n",
      "Found 12 images belonging to 1 classes.\n",
      "refuge fold_4 test normal\n",
      "Found 216 images belonging to 1 classes.\n",
      "refuge fold_4 test glaukoma\n",
      "Found 24 images belonging to 1 classes.\n",
      "refuge fold_5 val normal\n",
      "Found 108 images belonging to 1 classes.\n",
      "refuge fold_5 val glaukoma\n",
      "Found 12 images belonging to 1 classes.\n",
      "refuge fold_5 test normal\n",
      "Found 216 images belonging to 1 classes.\n",
      "refuge fold_5 test glaukoma\n",
      "Found 24 images belonging to 1 classes.\n",
      "papila fold_1 val normal\n",
      "Found 34 images belonging to 1 classes.\n",
      "papila fold_1 val glaukoma\n",
      "Found 15 images belonging to 1 classes.\n",
      "papila fold_1 test normal\n",
      "Found 67 images belonging to 1 classes.\n",
      "papila fold_1 test glaukoma\n",
      "Found 31 images belonging to 1 classes.\n",
      "papila fold_2 val normal\n",
      "Found 34 images belonging to 1 classes.\n",
      "papila fold_2 val glaukoma\n",
      "Found 15 images belonging to 1 classes.\n",
      "papila fold_2 test normal\n",
      "Found 67 images belonging to 1 classes.\n",
      "papila fold_2 test glaukoma\n",
      "Found 31 images belonging to 1 classes.\n",
      "papila fold_3 val normal\n",
      "Found 34 images belonging to 1 classes.\n",
      "papila fold_3 val glaukoma\n",
      "Found 15 images belonging to 1 classes.\n",
      "papila fold_3 test normal\n",
      "Found 67 images belonging to 1 classes.\n",
      "papila fold_3 test glaukoma\n",
      "Found 31 images belonging to 1 classes.\n",
      "papila fold_4 val normal\n",
      "Found 34 images belonging to 1 classes.\n",
      "papila fold_4 val glaukoma\n",
      "Found 15 images belonging to 1 classes.\n",
      "papila fold_4 test normal\n",
      "Found 66 images belonging to 1 classes.\n",
      "papila fold_4 test glaukoma\n",
      "Found 31 images belonging to 1 classes.\n",
      "papila fold_5 val normal\n",
      "Found 34 images belonging to 1 classes.\n",
      "papila fold_5 val glaukoma\n",
      "Found 15 images belonging to 1 classes.\n",
      "papila fold_5 test normal\n",
      "Found 66 images belonging to 1 classes.\n",
      "papila fold_5 test glaukoma\n",
      "Found 31 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# load image using image data generator for validation and test data\n",
    "for dataset in dataset_names:\n",
    "    for fold in fold_names:\n",
    "        for data_type in ['val', 'test']:\n",
    "            for label in labels_name:\n",
    "                print(f'{dataset} {fold} {data_type} {label}')\n",
    "                s3_val_test_src[dataset + '_'\n",
    "                                + fold + '_'\n",
    "                                + data_type + '_'\n",
    "                                + label] = (datagenerator_s3_clahe.flow_from_directory(\n",
    "                                            path_dataset_val_test_src[scenario_names[1] + '_'\n",
    "                                                                    + dataset + '_'\n",
    "                                                                    + fold + '_'\n",
    "                                                                    + data_type],\n",
    "                                            target_size=image_size[dataset],\n",
    "                                            class_mode='binary',\n",
    "                                            classes=[label],\n",
    "                                            shuffle=True,\n",
    "                                            seed=1915026018,\n",
    "                                            save_to_dir=path_dataset_val_test_dest[scenario_names[1] + '_'\n",
    "                                                                                    + dataset + '_'\n",
    "                                                                                    + fold + '_'\n",
    "                                                                                    + data_type + '_'\n",
    "                                                                                    + label],\n",
    "                                            save_prefix=f's3_{dataset}_f{fold.split(\"_\")[-1]}_{data_type}_{label}',\n",
    "                                            save_format='jpg'))\n",
    "                s3_cfile_val_test.append(f's3_{dataset}_f{fold.split(\"_\")[-1]}_{data_type}_{label}')\n",
    "del dataset, fold, data_type, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the augmented image & saved it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented image for rimone f1 normal...\n",
      "Elapsed time: 51.06 seconds\n",
      "Generating augmented image for rimone f1 glaukoma...\n",
      "Elapsed time: 20.56 seconds\n",
      "Generating augmented image for rimone f2 normal...\n",
      "Elapsed time: 26.37 seconds\n",
      "Generating augmented image for rimone f2 glaukoma...\n",
      "Elapsed time: 13.31 seconds\n",
      "Generating augmented image for rimone f3 normal...\n",
      "Elapsed time: 25.03 seconds\n",
      "Generating augmented image for rimone f3 glaukoma...\n",
      "Elapsed time: 14.95 seconds\n",
      "Generating augmented image for rimone f4 normal...\n",
      "Elapsed time: 25.64 seconds\n",
      "Generating augmented image for rimone f4 glaukoma...\n",
      "Elapsed time: 12.90 seconds\n",
      "Generating augmented image for rimone f5 normal...\n",
      "Elapsed time: 23.89 seconds\n",
      "Generating augmented image for rimone f5 glaukoma...\n",
      "Elapsed time: 13.01 seconds\n",
      "Generating augmented image for g1020 f1 normal...\n",
      "Elapsed time: 58.13 seconds\n",
      "Generating augmented image for g1020 f1 glaukoma...\n",
      "Elapsed time: 23.76 seconds\n",
      "Generating augmented image for g1020 f2 normal...\n",
      "Elapsed time: 58.49 seconds\n",
      "Generating augmented image for g1020 f2 glaukoma...\n",
      "Elapsed time: 26.66 seconds\n",
      "Generating augmented image for g1020 f3 normal...\n",
      "Elapsed time: 60.58 seconds\n",
      "Generating augmented image for g1020 f3 glaukoma...\n",
      "Elapsed time: 23.96 seconds\n",
      "Generating augmented image for g1020 f4 normal...\n",
      "Elapsed time: 58.99 seconds\n",
      "Generating augmented image for g1020 f4 glaukoma...\n",
      "Elapsed time: 23.95 seconds\n",
      "Generating augmented image for g1020 f5 normal...\n",
      "Elapsed time: 59.54 seconds\n",
      "Generating augmented image for g1020 f5 glaukoma...\n",
      "Elapsed time: 23.98 seconds\n",
      "Generating augmented image for refuge f1 normal...\n",
      "Elapsed time: 95.33 seconds\n",
      "Generating augmented image for refuge f1 glaukoma...\n",
      "Elapsed time: 10.58 seconds\n",
      "Generating augmented image for refuge f2 normal...\n",
      "Elapsed time: 95.43 seconds\n",
      "Generating augmented image for refuge f2 glaukoma...\n",
      "Elapsed time: 10.72 seconds\n",
      "Generating augmented image for refuge f3 normal...\n",
      "Elapsed time: 99.43 seconds\n",
      "Generating augmented image for refuge f3 glaukoma...\n",
      "Elapsed time: 10.68 seconds\n",
      "Generating augmented image for refuge f4 normal...\n",
      "Elapsed time: 102.00 seconds\n",
      "Generating augmented image for refuge f4 glaukoma...\n",
      "Elapsed time: 10.99 seconds\n",
      "Generating augmented image for refuge f5 normal...\n",
      "Elapsed time: 108.00 seconds\n",
      "Generating augmented image for refuge f5 glaukoma...\n",
      "Elapsed time: 11.09 seconds\n",
      "Generating augmented image for papila f1 normal...\n",
      "Elapsed time: 23.51 seconds\n",
      "Generating augmented image for papila f1 glaukoma...\n",
      "Elapsed time: 11.10 seconds\n",
      "Generating augmented image for papila f2 normal...\n",
      "Elapsed time: 23.32 seconds\n",
      "Generating augmented image for papila f2 glaukoma...\n",
      "Elapsed time: 11.18 seconds\n",
      "Generating augmented image for papila f3 normal...\n",
      "Elapsed time: 23.28 seconds\n",
      "Generating augmented image for papila f3 glaukoma...\n",
      "Elapsed time: 11.30 seconds\n",
      "Generating augmented image for papila f4 normal...\n",
      "Elapsed time: 23.36 seconds\n",
      "Generating augmented image for papila f4 glaukoma...\n",
      "Elapsed time: 10.97 seconds\n",
      "Generating augmented image for papila f5 normal...\n",
      "Elapsed time: 23.35 seconds\n",
      "Generating augmented image for papila f5 glaukoma...\n",
      "Elapsed time: 11.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# generate the augmented image for training data\n",
    "augment_image.generate_aug_img(dataset_names=dataset_names,\n",
    "                                fold_names=fold_names,\n",
    "                                labels_names=labels_name,\n",
    "                                batch_datasets=s3_src,\n",
    "                                data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented image for rimone f1 normal...\n",
      "Elapsed time: 24.98 seconds\n",
      "Generating augmented image for rimone f1 glaukoma...\n",
      "Elapsed time: 13.25 seconds\n",
      "Generating augmented image for rimone f2 normal...\n",
      "Elapsed time: 23.80 seconds\n",
      "Generating augmented image for rimone f2 glaukoma...\n",
      "Elapsed time: 13.46 seconds\n",
      "Generating augmented image for rimone f3 normal...\n",
      "Elapsed time: 27.99 seconds\n",
      "Generating augmented image for rimone f3 glaukoma...\n",
      "Elapsed time: 15.60 seconds\n",
      "Generating augmented image for rimone f4 normal...\n",
      "Elapsed time: 26.03 seconds\n",
      "Generating augmented image for rimone f4 glaukoma...\n",
      "Elapsed time: 13.28 seconds\n",
      "Generating augmented image for rimone f5 normal...\n",
      "Elapsed time: 23.96 seconds\n",
      "Generating augmented image for rimone f5 glaukoma...\n",
      "Elapsed time: 13.39 seconds\n",
      "Generating augmented image for g1020 f1 normal...\n",
      "Elapsed time: 60.24 seconds\n",
      "Generating augmented image for g1020 f1 glaukoma...\n",
      "Elapsed time: 24.33 seconds\n",
      "Generating augmented image for g1020 f2 normal...\n",
      "Elapsed time: 59.89 seconds\n",
      "Generating augmented image for g1020 f2 glaukoma...\n",
      "Elapsed time: 24.34 seconds\n",
      "Generating augmented image for g1020 f3 normal...\n",
      "Elapsed time: 83.78 seconds\n",
      "Generating augmented image for g1020 f3 glaukoma...\n",
      "Elapsed time: 26.54 seconds\n",
      "Generating augmented image for g1020 f4 normal...\n",
      "Elapsed time: 59.76 seconds\n",
      "Generating augmented image for g1020 f4 glaukoma...\n",
      "Elapsed time: 40.04 seconds\n",
      "Generating augmented image for g1020 f5 normal...\n",
      "Elapsed time: 94.51 seconds\n",
      "Generating augmented image for g1020 f5 glaukoma...\n",
      "Elapsed time: 24.64 seconds\n",
      "Generating augmented image for refuge f1 normal...\n",
      "Elapsed time: 107.30 seconds\n",
      "Generating augmented image for refuge f1 glaukoma...\n",
      "Elapsed time: 11.98 seconds\n",
      "Generating augmented image for refuge f2 normal...\n",
      "Elapsed time: 129.15 seconds\n",
      "Generating augmented image for refuge f2 glaukoma...\n",
      "Elapsed time: 13.81 seconds\n",
      "Generating augmented image for refuge f3 normal...\n",
      "Elapsed time: 127.18 seconds\n",
      "Generating augmented image for refuge f3 glaukoma...\n",
      "Elapsed time: 10.74 seconds\n",
      "Generating augmented image for refuge f4 normal...\n",
      "Elapsed time: 154.36 seconds\n",
      "Generating augmented image for refuge f4 glaukoma...\n",
      "Elapsed time: 18.40 seconds\n",
      "Generating augmented image for refuge f5 normal...\n",
      "Elapsed time: 123.16 seconds\n",
      "Generating augmented image for refuge f5 glaukoma...\n",
      "Elapsed time: 11.73 seconds\n",
      "Generating augmented image for papila f1 normal...\n",
      "Elapsed time: 27.40 seconds\n",
      "Generating augmented image for papila f1 glaukoma...\n",
      "Elapsed time: 12.33 seconds\n",
      "Generating augmented image for papila f2 normal...\n",
      "Elapsed time: 33.43 seconds\n",
      "Generating augmented image for papila f2 glaukoma...\n",
      "Elapsed time: 16.37 seconds\n",
      "Generating augmented image for papila f3 normal...\n",
      "Elapsed time: 30.51 seconds\n",
      "Generating augmented image for papila f3 glaukoma...\n",
      "Elapsed time: 15.82 seconds\n",
      "Generating augmented image for papila f4 normal...\n",
      "Elapsed time: 33.73 seconds\n",
      "Generating augmented image for papila f4 glaukoma...\n",
      "Elapsed time: 14.13 seconds\n",
      "Generating augmented image for papila f5 normal...\n",
      "Elapsed time: 31.34 seconds\n",
      "Generating augmented image for papila f5 glaukoma...\n",
      "Elapsed time: 14.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# generate the augmented image for training data with only clahe augmentation\n",
    "augment_image.generate_aug_img(dataset_names=dataset_names,\n",
    "                                fold_names=fold_names,\n",
    "                                labels_names=labels_name,\n",
    "                                batch_datasets=s3_tclahe_src,\n",
    "                                data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented image for rimone f1 val...\n",
      "Elapsed time: 6.52 seconds\n",
      "Generating augmented image for rimone f1 test...\n",
      "Elapsed time: 11.93 seconds\n",
      "Generating augmented image for rimone f2 val...\n",
      "Elapsed time: 6.20 seconds\n",
      "Generating augmented image for rimone f2 test...\n",
      "Elapsed time: 11.46 seconds\n",
      "Generating augmented image for rimone f3 val...\n",
      "Elapsed time: 5.61 seconds\n",
      "Generating augmented image for rimone f3 test...\n",
      "Elapsed time: 11.45 seconds\n",
      "Generating augmented image for rimone f4 val...\n",
      "Elapsed time: 6.82 seconds\n",
      "Generating augmented image for rimone f4 test...\n",
      "Elapsed time: 11.93 seconds\n",
      "Generating augmented image for rimone f5 val...\n",
      "Elapsed time: 8.07 seconds\n",
      "Generating augmented image for rimone f5 test...\n",
      "Elapsed time: 12.43 seconds\n",
      "Generating augmented image for g1020 f1 val...\n",
      "Elapsed time: 18.06 seconds\n",
      "Generating augmented image for g1020 f1 test...\n",
      "Elapsed time: 27.19 seconds\n",
      "Generating augmented image for g1020 f2 val...\n",
      "Elapsed time: 13.34 seconds\n",
      "Generating augmented image for g1020 f2 test...\n",
      "Elapsed time: 26.83 seconds\n",
      "Generating augmented image for g1020 f3 val...\n",
      "Elapsed time: 11.91 seconds\n",
      "Generating augmented image for g1020 f3 test...\n",
      "Elapsed time: 24.07 seconds\n",
      "Generating augmented image for g1020 f4 val...\n",
      "Elapsed time: 12.27 seconds\n",
      "Generating augmented image for g1020 f4 test...\n",
      "Elapsed time: 24.30 seconds\n",
      "Generating augmented image for g1020 f5 val...\n",
      "Elapsed time: 12.01 seconds\n",
      "Generating augmented image for g1020 f5 test...\n",
      "Elapsed time: 24.13 seconds\n",
      "Generating augmented image for refuge f1 val...\n",
      "Elapsed time: 14.66 seconds\n",
      "Generating augmented image for refuge f1 test...\n",
      "Elapsed time: 29.94 seconds\n",
      "Generating augmented image for refuge f2 val...\n",
      "Elapsed time: 21.59 seconds\n",
      "Generating augmented image for refuge f2 test...\n",
      "Elapsed time: 37.31 seconds\n",
      "Generating augmented image for refuge f3 val...\n",
      "Elapsed time: 18.90 seconds\n",
      "Generating augmented image for refuge f3 test...\n",
      "Elapsed time: 43.22 seconds\n",
      "Generating augmented image for refuge f4 val...\n",
      "Elapsed time: 26.18 seconds\n",
      "Generating augmented image for refuge f4 test...\n",
      "Elapsed time: 39.13 seconds\n",
      "Generating augmented image for refuge f5 val...\n",
      "Elapsed time: 17.00 seconds\n",
      "Generating augmented image for refuge f5 test...\n",
      "Elapsed time: 40.65 seconds\n",
      "Generating augmented image for papila f1 val...\n",
      "Elapsed time: 7.26 seconds\n",
      "Generating augmented image for papila f1 test...\n",
      "Elapsed time: 13.32 seconds\n",
      "Generating augmented image for papila f2 val...\n",
      "Elapsed time: 6.67 seconds\n",
      "Generating augmented image for papila f2 test...\n",
      "Elapsed time: 13.12 seconds\n",
      "Generating augmented image for papila f3 val...\n",
      "Elapsed time: 6.42 seconds\n",
      "Generating augmented image for papila f3 test...\n",
      "Elapsed time: 13.25 seconds\n",
      "Generating augmented image for papila f4 val...\n",
      "Elapsed time: 5.74 seconds\n",
      "Generating augmented image for papila f4 test...\n",
      "Elapsed time: 11.08 seconds\n",
      "Generating augmented image for papila f5 val...\n",
      "Elapsed time: 5.68 seconds\n",
      "Generating augmented image for papila f5 test...\n",
      "Elapsed time: 11.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# generate the augmented image for validation and testing data\n",
    "augment_image.generate_aug_img(dataset_names=dataset_names,\n",
    "                                fold_names=fold_names,\n",
    "                                labels_names=labels_name,\n",
    "                                batch_datasets=s3_val_test_src,\n",
    "                                data_type='val_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a varible to store the file name\n",
    "s3_src_fname = {}\n",
    "s3_aug_fname = {}\n",
    "\n",
    "# collecting the source file name\n",
    "for key, value in path_dataset_src.items():\n",
    "    if key.split('_')[1] == '3':\n",
    "        for label in labels_name:\n",
    "            s3_src_fname[key + '_'\n",
    "                        + label] = [file for file in os.listdir(os.path.join(value,label))]\n",
    "del key, value, label\n",
    "\n",
    "# collecting the augmented file name\n",
    "for key, value in path_dataset_aug.items():\n",
    "    if key.split('_')[1] == '3':\n",
    "        s3_aug_fname[key] = [file for file in os.listdir(value)]\n",
    "del key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>file_count</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scenario_3_rimone_fold_1_normal</td>\n",
       "      <td>436</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scenario_3_rimone_fold_1_glaukoma</td>\n",
       "      <td>242</td>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            category  file_count    type\n",
       "0    scenario_3_rimone_fold_1_normal         436  source\n",
       "1  scenario_3_rimone_fold_1_glaukoma         242  source"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the two dictionary into dataframe\n",
    "s3_df_result = pd.concat([pd.DataFrame({\n",
    "                                'category': s3_src_fname.keys(),\n",
    "                                'file_count': [len(value) for value in s3_src_fname.values()],\n",
    "                                'type': 'source'\n",
    "                            }),\n",
    "                            pd.DataFrame({\n",
    "                                'category': s3_aug_fname.keys(),\n",
    "                                'file_count': [len(value) for value in s3_aug_fname.values()],\n",
    "                                'type': 'augmented'\n",
    "                            })])\n",
    "s3_df_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>augmented</th>\n",
       "      <th>source</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scenario_3_g1020_fold_1_glaukoma</th>\n",
       "      <td>208.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_3_g1020_fold_1_normal</th>\n",
       "      <td>506.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_3_g1020_fold_2_glaukoma</th>\n",
       "      <td>208.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_3_g1020_fold_2_normal</th>\n",
       "      <td>506.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_3_g1020_fold_3_glaukoma</th>\n",
       "      <td>208.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type                              augmented  source status\n",
       "category                                                  \n",
       "scenario_3_g1020_fold_1_glaukoma      208.0   416.0  valid\n",
       "scenario_3_g1020_fold_1_normal        506.0  1012.0  valid\n",
       "scenario_3_g1020_fold_2_glaukoma      208.0   416.0  valid\n",
       "scenario_3_g1020_fold_2_normal        506.0  1012.0  valid\n",
       "scenario_3_g1020_fold_3_glaukoma      208.0   416.0  valid"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the file count\n",
    "s3_df_validate = pd.DataFrame(s3_df_result.groupby(['category', 'type']).file_count.sum())\n",
    "\n",
    "s3_df_validate.sort_values(by='category', inplace=True)\n",
    "s3_df_validate = s3_df_validate.pivot_table(index='category',\n",
    "                                            columns='type',\n",
    "                                            values='file_count')\n",
    "\n",
    "s3_df_validate.loc[s3_df_validate.augmented*2 == s3_df_validate.source,\n",
    "                    'status'] = 'valid'\n",
    "s3_df_validate.loc[s3_df_validate.augmented*2 != s3_df_validate.source,\n",
    "                    'status'] = 'invalid'\n",
    "\n",
    "s3_df_validate.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 40\n",
      "\n",
      "Total valid categories: 40\n",
      "\n",
      "Total invalid categories: 0\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(f'Total categories: {len(s3_df_validate)}',\n",
    "    f'\\nTotal valid categories: {len(s3_df_validate[s3_df_validate.status == \"valid\"])}',\n",
    "    f'\\nTotal invalid categories: {len(s3_df_validate[s3_df_validate.status == \"invalid\"])}',\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the previous image\n",
    "##### Getting the image file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable to store the file name for validation and testing data\n",
    "s3_rm_file_vt, s3_aug_file_vt = augment_image.get_file(files_code=s3_cfile_val_test,\n",
    "                                    path_dest=path_dataset_val_test_dest,\n",
    "                                    scenario=scenario_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable to store the file name for training data with only clahe\n",
    "s3_rm_file_tc, s3_aug_file_tc = augment_image.get_file(files_code=s3_cfile_train,\n",
    "                                    path_dest=path_dataset_clahe_dst,\n",
    "                                    scenario=scenario_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe to store the result\n",
    "s3_df_file_check = pd.DataFrame(columns = ['data',\n",
    "                                        'type',\n",
    "                                        'category',\n",
    "                                        'file_path',\n",
    "                                        'file_name'])\n",
    "s3_df_file_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the file name that will be removed result into the dataframe\n",
    "for category, files_list in s3_rm_file_vt.items():\n",
    "    for file in files_list:\n",
    "        s3_df_file_check.loc[len(s3_df_file_check)] = ['val_test',\n",
    "                                                    'remove',\n",
    "                                                    category[3:].replace('_', ' '),\n",
    "                                                    file,\n",
    "                                                    os.path.basename(file)]\n",
    "del category, files_list, file\n",
    "# add the file name that is augmented into the dataframe\n",
    "for category, files_list in s3_aug_file_vt.items():\n",
    "    for file in files_list:\n",
    "        s3_df_file_check.loc[len(s3_df_file_check)] = ['val_test',\n",
    "                                                    'augment',\n",
    "                                                    category[3:].replace('_', ' '),\n",
    "                                                    file,\n",
    "                                                    os.path.basename(file)]\n",
    "del category, files_list, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the file name that will be removed result into the dataframe\n",
    "for category, files_list in s3_rm_file_tc.items():\n",
    "    for file in files_list:\n",
    "        s3_df_file_check.loc[len(s3_df_file_check)] = ['train_clahe',\n",
    "                                                    'remove',\n",
    "                                                    category[3:].replace('_', ' '),\n",
    "                                                    file,\n",
    "                                                    os.path.basename(file)]\n",
    "del category, files_list, file\n",
    "# add the file name that is augmented into the dataframe\n",
    "for category, files_list in s3_aug_file_tc.items():\n",
    "    for file in files_list:\n",
    "        s3_df_file_check.loc[len(s3_df_file_check)] = ['train_clahe',\n",
    "                                                    'augment',\n",
    "                                                    category[3:].replace('_', ' '),\n",
    "                                                    file,\n",
    "                                                    os.path.basename(file)]\n",
    "del category, files_list, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the result into the compare able shape for validation and testing\n",
    "s3_df_validate_vt = pd.DataFrame(s3_df_file_check.loc[s3_df_file_check['data'] == 'val_test'].groupby(by=['category',\n",
    "                                                                                    'type']).count()['file_name'])\n",
    "\n",
    "\n",
    "s3_df_validate_vt = s3_df_validate_vt.pivot_table(values='file_name',\n",
    "                                            index='category',\n",
    "                                            columns='type')\n",
    "s3_df_validate_vt.loc[s3_df_validate_vt.augment == s3_df_validate_vt.remove,\n",
    "                    'status'] = 'valid'\n",
    "s3_df_validate_vt.loc[s3_df_validate_vt.augment != s3_df_validate_vt.remove,\n",
    "                    'status'] = 'invalid'\n",
    "s3_df_validate_vt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the result into the compare able shape for training augmentation with only clahe\n",
    "s3_df_validate_vt = pd.DataFrame(s3_df_file_check.loc[s3_df_file_check['data'] == 'train_clahe'].groupby(by=['category',\n",
    "                                                                                    'type']).count()['file_name'])\n",
    "\n",
    "\n",
    "s3_df_validate_vt = s3_df_validate_vt.pivot_table(values='file_name',\n",
    "                                            index='category',\n",
    "                                            columns='type')\n",
    "s3_df_validate_vt.loc[s3_df_validate_vt.augment == s3_df_validate_vt.remove,\n",
    "                    'status'] = 'valid'\n",
    "s3_df_validate_vt.loc[s3_df_validate_vt.augment != s3_df_validate_vt.remove,\n",
    "                    'status'] = 'invalid'\n",
    "s3_df_validate_vt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(f'total categories: {s3_df_validate.shape[0]}',\n",
    "        f'valid file(s)   : {s3_df_validate.loc[s3_df_validate.status == \"valid\"].shape[0]}',\n",
    "        f'invalid file(s) : {s3_df_validate.loc[s3_df_validate.status == \"invalid\"].shape[0]}',\n",
    "        sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in s3_rm_file.values():\n",
    "    result_status = augment_image.remove_file(files)\n",
    "\n",
    "print(f'Files removed: {len(result_status[\"Success\"])}',\n",
    "        f'Files already removed: {len(result_status[\"Not Found\"])}',\n",
    "        sep='\\n')\n",
    "\n",
    "del files, result_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing scenario one augmented directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rmdir = []\n",
    "for scenario in scenario_names:\n",
    "    for dataset in dataset_names:\n",
    "        for fold in fold_names:\n",
    "            s1_rmdir.append(os.path.join(path_source,\n",
    "                                        scenario,\n",
    "                                        dataset,\n",
    "                                        fold,\n",
    "                                        'train_augmented'))\n",
    "    break\n",
    "\n",
    "result_status = augment_image.remove_dir(s1_rmdir)\n",
    "print(f'Directory removed: {len(result_status[\"Success\"])}',\n",
    "        f'Directory already removed: {len(result_status[\"Not Found\"])}',\n",
    "        sep='\\n')\n",
    "\n",
    "del scenario, dataset, fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
